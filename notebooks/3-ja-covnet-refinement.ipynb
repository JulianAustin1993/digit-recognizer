{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining covnet structure\n",
    "\n",
    "From [exploration](./2-ja-covnet.ipynb) we have found a structure:\n",
    "\n",
    "**784 - 32C5-32C5S2-64C5-64C5S2 - 128 - 10**\n",
    "\n",
    "which performs reasonably well on validation data set (>99%).\n",
    "\n",
    "This noteboook will explore variations on this structure for improvements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename=\"mylog.log\", mode=\"a\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Setting up logging...\")\n",
    "\n",
    "\n",
    "from src.utils import CustomMnistDataset, train_covnet\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "SEED = int(os.getenv(\"SEED\"))  # type: ignore\n",
    "TENSORBOARD_DIR = Path(os.getenv(\"TENSORBOARD_DIR\"))\n",
    "MODEL_DIR = Path(os.getenv(\"MODEL_DIR\"))\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "augmentor = nn.Sequential(\n",
    "    T.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    ")\n",
    "train_dataset = CustomMnistDataset(\n",
    "    img_dir=DATA_DIR, type=\"train\", transform=torch.jit.script(augmentor)\n",
    ")\n",
    "val_dataset = CustomMnistDataset(img_dir=DATA_DIR, type=\"validation\")\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "SHUFFLE = True\n",
    "EPOCHS = 50\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "optimizer_wrapper = lambda x: optim.SGD(x, lr=0.01, momentum=0.90)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **784 - 32C5-32C5S2-64C5-64C5S2- 128 - 10** with Batch normalization and Dropout. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name='32C5-32C5S2-64C5-64C5S2-128-10-BN-DO'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # 32, 24, 24, \n",
    "        self.pool1 = nn.Conv2d(32, 32, 5, 2, padding=2) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) \n",
    "        self.pool2 = nn.Conv2d(64, 64, 5, 2, padding=2)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn1b(self.pool1(x)))\n",
    "        x = self.dropout(x) #(32, 12, 12)\n",
    "        # layer 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn2b(self.pool2(x)))\n",
    "        x = self.dropout(x) #(64, 4, 4)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.dropout(F.relu(self.bn3(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "  \n",
    "train_dict = train_covnet(\n",
    "    net = Net(),\n",
    "    dataloader = train_dataloader,\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **784 - 32C3-32C3-32C5S2-64C3-64C3-64C5S2- 128 - 10** with Batch normalization and Dropout. \n",
    "\n",
    "Replacing the 5x5 convolution layer with two 3x3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name='32C3-32C3-32C5S2-64C3-64C3-64C5S2-128-10-BN-DO'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3) \n",
    "        self.conv1a = nn.Conv2d(32,32,3) # 32, 24, 24, \n",
    "        self.pool1 = nn.Conv2d(32, 32, 5, 2, padding=2) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv2a = nn.Conv2d(64, 64, 3) \n",
    "        self.pool2 = nn.Conv2d(64, 64, 5, 2, padding=2)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn1a = nn.BatchNorm2d(32)\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn2a = nn.BatchNorm2d(64)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.pool1(x)))\n",
    "        x = self.dropout(x) #(32, 12, 12)\n",
    "        # layer 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.pool2(x)))\n",
    "        x = self.dropout(x) #(64, 4, 4)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.dropout(F.relu(self.bn3(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "  \n",
    "train_dict = train_covnet(\n",
    "    net = Net(),\n",
    "    dataloader = train_dataloader,\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **784 - 32C5-32C5S2-64C5-64C5S2- 128 - 10** with Batch normalization and Dropout per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name='32C5-32C5S2-D1-64C5-64C5S2-D2-128-D3-10-BN'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # 32, 24, 24, \n",
    "        self.pool1 = nn.Conv2d(32, 32, 5, 2, padding=2) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) \n",
    "        self.pool2 = nn.Conv2d(64, 64, 5, 2, padding=2)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.d1 = nn.Dropout(p=0.1)\n",
    "        self.d2 = nn.Dropout(p=0.25)\n",
    "        self.d3 = nn.Dropout(p=0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn1b(self.pool1(x)))\n",
    "        x = self.d1(x) #(32, 12, 12)\n",
    "        # layer 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn2b(self.pool2(x)))\n",
    "        x = self.d2(x) #(64, 4, 4)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.d3(F.relu(self.bn3(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "  \n",
    "train_dict = train_covnet(\n",
    "    net = Net(),\n",
    "    dataloader = train_dataloader,\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **784 - 32C5-32C5S2-64C5-64C5S2- 128C4 - 10** with Batch normalization and Dropout per layer with Test Time Augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name='32C5-32C5S2-D1-64C5-64C5S2-D2-128C4-D3-10-BN-TTA'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # 32, 24, 24, \n",
    "        self.pool1 = nn.Conv2d(32, 32, 5, 2, padding=2) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) \n",
    "        self.pool2 = nn.Conv2d(64, 64, 5, 2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.d1 = nn.Dropout(p=0.1)\n",
    "        self.d2 = nn.Dropout(p=0.25)\n",
    "        self.d3 = nn.Dropout(p=0.4)\n",
    "        self.augmentor = augmentor\n",
    "        self.n_augs = 8\n",
    "        \n",
    "\n",
    "    def forward_model(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn1b(self.pool1(x)))\n",
    "        x = self.d1(x) #(32, 12, 12)\n",
    "        # layer 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn2b(self.pool2(x)))\n",
    "        x = self.d2(x) #(64, 4, 4)\n",
    "        # layer 3\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.d3(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.forward_model(x)\n",
    "        if self.train:\n",
    "            return out\n",
    "        for _ in range(self.n_augs):\n",
    "            out += self.forward_model(self.augmentor(x))\n",
    "        return out / (self.n_augs + 1)\n",
    "\n",
    "  \n",
    "train_dict = train_covnet(\n",
    "    net = Net(),\n",
    "    dataloader = train_dataloader,\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13838a53fc594ffa30a34b5031f03b2aa916b9b3ee5245d7f75d470d94945eb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
