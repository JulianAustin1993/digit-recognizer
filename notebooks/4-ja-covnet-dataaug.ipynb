{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of data augmentation on Performance.\n",
    "From [exploration](./3-ja-covnet-refinement.ipynb) we have found a structure:\n",
    "\n",
    "**784 - 32C5-32C5S2-64C5-64C5S2- 128C4 - 10**\n",
    "\n",
    "which performs reasonably well on validation data set (>99.4%).\n",
    "\n",
    "Here we will test different data augmentation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename=\"mylog.log\", mode=\"a\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Setting up logging...\")\n",
    "\n",
    "\n",
    "from src.utils import CustomMnistDataset, train_covnet\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "SEED = int(os.getenv(\"SEED\"))  # type: ignore\n",
    "TENSORBOARD_DIR = Path(os.getenv(\"TENSORBOARD_DIR\"))\n",
    "MODEL_DIR = Path(os.getenv(\"MODEL_DIR\"))\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BATCH_SIZE = 20\n",
    "SHUFFLE = True\n",
    "EPOCHS = 25\n",
    "optimizer_wrapper = lambda x: optim.SGD(x, lr=0.01, momentum=0.90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "**784 - 32C5-32C5S2-64C5-64C5S2- 128C4 - 10** with Batch normalization and Dropout per layer with Test Time Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, augmentor):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # 32, 24, 24, \n",
    "        self.pool1 = nn.Conv2d(32, 32, 5, 2, padding=2) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) \n",
    "        self.pool2 = nn.Conv2d(64, 64, 5, 2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.d1 = nn.Dropout(p=0.1)\n",
    "        self.d2 = nn.Dropout(p=0.25)\n",
    "        self.d3 = nn.Dropout(p=0.4)\n",
    "        self.augmentor = augmentor\n",
    "        self.n_augs = 16\n",
    "        \n",
    "\n",
    "    def forward_model(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn1b(self.pool1(x)))\n",
    "        x = self.d1(x) #(32, 12, 12)\n",
    "        # layer 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn2b(self.pool2(x)))\n",
    "        x = self.d2(x) #(64, 4, 4)\n",
    "        # layer 3\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.d3(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.forward_model(x)\n",
    "        if self.train:\n",
    "            return out\n",
    "        for _ in range(self.n_augs):\n",
    "            out += self.forward_model(self.augmentor(x))\n",
    "        return out / (self.n_augs + 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '32C5-32C5S2-D1-64C5-64C5S2-D2-128C4-D3-10-BN-TTA-AFFINE-SMALL'\n",
    "augmentor = nn.Sequential(\n",
    "    T.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05))\n",
    ")\n",
    "scripted_augmentor = torch.jit.script(augmentor) #type: ignore\n",
    "train_dataset = CustomMnistDataset(\n",
    "    img_dir=DATA_DIR, type=\"train\", transform=scripted_augmentor\n",
    ")\n",
    "val_dataset = CustomMnistDataset(img_dir=DATA_DIR, type=\"validation\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "train_dict = train_covnet(\n",
    "    net = Net(scripted_augmentor),\n",
    "    dataloader = train_dataloader,\n",
    "\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) ) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '32C5-32C5S2-D1-64C5-64C5S2-D2-128C4-D3-10-BN-TTA-AFFINE-LARGE'\n",
    "augmentor = nn.Sequential(\n",
    "    T.RandomAffine(degrees=22.5, translate=(0.2, 0.2), scale=(0.8, 1.2))\n",
    ")\n",
    "scripted_augmentor = torch.jit.script(augmentor) #type: ignore\n",
    "train_dataset = CustomMnistDataset(\n",
    "    img_dir=DATA_DIR, type=\"train\", transform=scripted_augmentor\n",
    ")\n",
    "val_dataset = CustomMnistDataset(img_dir=DATA_DIR, type=\"validation\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "train_dict = train_covnet(\n",
    "    net = Net(scripted_augmentor),\n",
    "    dataloader = train_dataloader,\n",
    "\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) ) #type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '32C5-32C5S2-D1-64C5-64C5S2-D2-128C4-D3-10-BN-TTA-ELASTIC-SMALL'\n",
    "augmentor = nn.Sequential(\n",
    "    T.ElasticTransform(alpha=50.0, sigma=5.0)\n",
    ")\n",
    "scripted_augmentor = augmentor #type: ignore\n",
    "train_dataset = CustomMnistDataset(\n",
    "    img_dir=DATA_DIR, type=\"train\", transform=scripted_augmentor\n",
    ")\n",
    "val_dataset = CustomMnistDataset(img_dir=DATA_DIR, type=\"validation\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "train_dict = train_covnet(\n",
    "    net = Net(scripted_augmentor),\n",
    "    dataloader = train_dataloader,\n",
    "\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) ) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '32C5-32C5S2-D1-64C5-64C5S2-D2-128C4-D3-10-BN-TTA-ELASTIC-LARGE'\n",
    "augmentor = nn.Sequential(\n",
    "    T.RandomAffine(degrees=, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "    T.ElasticTransform(alpha=100.0, sigma=5.0)\n",
    ")\n",
    "scripted_augmentor = augmentor #type: ignore\n",
    "train_dataset = CustomMnistDataset(\n",
    "    img_dir=DATA_DIR, type=\"train\", transform=scripted_augmentor\n",
    ")\n",
    "val_dataset = CustomMnistDataset(img_dir=DATA_DIR, type=\"validation\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "train_dict = train_covnet(\n",
    "    net = Net(scripted_augmentor),\n",
    "    dataloader = train_dataloader,\n",
    "\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) ) #type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine + Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '32C5-32C5S2-D1-64C5-64C5S2-D2-128C4-D3-10-BN-TTA-ELASTIC-AFFINE-SMALL'\n",
    "augmentor = nn.Sequential(\n",
    "    T.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    T.ElasticTransform(alpha=30.0, sigma=5.0)\n",
    ")\n",
    "scripted_augmentor = augmentor #type: ignore\n",
    "train_dataset = CustomMnistDataset(\n",
    "    img_dir=DATA_DIR, type=\"train\", transform=scripted_augmentor\n",
    ")\n",
    "val_dataset = CustomMnistDataset(img_dir=DATA_DIR, type=\"validation\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "train_dict = train_covnet(\n",
    "    net = Net(scripted_augmentor),\n",
    "    dataloader = train_dataloader,\n",
    "\n",
    "    epochs = EPOCHS,\n",
    "    optimizer_wrapper = optimizer_wrapper,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    writer = SummaryWriter(Path(TENSORBOARD_DIR, exp_name)), #type; ignore\n",
    "    val_dataloader = val_dataloader\n",
    ")\n",
    "torch.save(train_dict.get('model').state_dict(), Path(MODEL_DIR, exp_name) ) #type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(augmentor)\n",
    "model.load_state_dict(torch.load(Path(MODEL_DIR, exp_name)))\n",
    "model.eval()\n",
    "preds = []\n",
    "test_dataset = CustomMnistDataset(img_dir=DATA_DIR, type=\"test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "for inputs, in iter(test_dataloader):\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    preds.extend(predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "prediction_df = pd.DataFrame({\"ImageId\": range(1,len(preds)+1), \"Label\": preds})\n",
    "prediction_df.to_csv(Path(DATA_DIR) / ('../'+exp_name +'_submission.csv' ), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13838a53fc594ffa30a34b5031f03b2aa916b9b3ee5245d7f75d470d94945eb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
